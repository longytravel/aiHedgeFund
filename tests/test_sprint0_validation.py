"""
Sprint 0 Validation Tests

These tests validate that the Sprint 0 setup is complete and working correctly.

Run with: pytest tests/test_sprint0_validation.py -v
"""

import pytest


# ============================================================================
# Sprint 0 Setup Validation
# ============================================================================


class TestSprintZeroSetup:
    """Validate Sprint 0 prerequisites are working."""

    def test_python_version(self):
        """Verify Python 3.14+ is installed."""
        import sys

        assert sys.version_info >= (3, 14), "Python 3.14+ required"

    def test_imports_core_dependencies(self):
        """Verify core dependencies can be imported."""
        try:
            import fastapi
            import langchain
            import langgraph
            import sqlalchemy
            import pydantic

            assert True, "All core dependencies imported successfully"
        except ImportError as e:
            pytest.fail(f"Failed to import core dependency: {e}")

    def test_imports_ai_dependencies(self):
        """Verify AI/ML dependencies can be imported."""
        try:
            import openai
            import anthropic
            # import google.generativeai  # May not be installed yet

            assert True, "AI dependencies imported successfully"
        except ImportError as e:
            pytest.fail(f"Failed to import AI dependency: {e}")

    def test_imports_test_dependencies(self):
        """Verify test dependencies are available."""
        try:
            import pytest
            import pytest_asyncio
            import pytest_mock
            import faker

            assert True, "Test dependencies imported successfully"
        except ImportError as e:
            pytest.fail(f"Failed to import test dependency: {e}")


# ============================================================================
# Mock LLM Provider Tests (HP-01)
# ============================================================================


class TestMockLLMProvider:
    """Validate Mock LLM provider works correctly."""

    @pytest.mark.asyncio
    async def test_mock_llm_default_response(self, mock_llm):
        """Test Mock LLM returns default response."""
        response = await mock_llm.generate("Test prompt")

        assert response.content == "MOCK LLM RESPONSE"
        assert response.model == "mock-gpt-4"
        assert len(mock_llm.call_history) == 1

    @pytest.mark.asyncio
    async def test_mock_llm_custom_response(self, mock_llm):
        """Test Mock LLM returns custom response for keywords."""
        mock_llm.set_response("bullish", "Stock looks great! BUY recommendation.")

        response = await mock_llm.generate("Is this stock bullish?")

        assert "BUY" in response.content
        assert response.content == "Stock looks great! BUY recommendation."

    @pytest.mark.asyncio
    async def test_mock_llm_bullish_fixture(self, mock_llm_bullish):
        """Test bullish analysis fixture."""
        response = await mock_llm_bullish.generate("Analyze BARC.LSE")

        assert "BULLISH" in response.content
        assert "BUY" in response.content
        assert "8/10" in response.content

    @pytest.mark.asyncio
    async def test_mock_llm_bearish_fixture(self, mock_llm_bearish):
        """Test bearish analysis fixture."""
        response = await mock_llm_bearish.generate("Analyze XYZ.LSE")

        assert "BEARISH" in response.content
        assert "SELL" in response.content
        assert "7/10" in response.content

    @pytest.mark.asyncio
    async def test_mock_llm_call_history(self, mock_llm):
        """Test call history tracking."""
        await mock_llm.generate("First prompt", model="gpt-4", temperature=0.5)
        await mock_llm.generate("Second prompt", model="claude-3", temperature=0.7)

        assert len(mock_llm.call_history) == 2
        assert mock_llm.call_history[0]["prompt"] == "First prompt"
        assert mock_llm.call_history[0]["model"] == "gpt-4"
        assert mock_llm.call_history[0]["temperature"] == 0.5
        assert mock_llm.call_history[1]["prompt"] == "Second prompt"

    @pytest.mark.asyncio
    async def test_mock_llm_reset(self, mock_llm):
        """Test reset clears history and custom responses."""
        mock_llm.set_response("test", "custom response")
        await mock_llm.generate("test prompt")

        assert len(mock_llm.call_history) == 1

        mock_llm.reset()

        assert len(mock_llm.call_history) == 0
        response = await mock_llm.generate("test prompt")
        assert response.content == "MOCK LLM RESPONSE"  # Back to default


# ============================================================================
# Test Data Fixtures Tests
# ============================================================================


class TestFixtures:
    """Validate test fixtures work correctly."""

    def test_sample_signal_factory(self, sample_signal):
        """Test signal factory creates valid signals."""
        signal = sample_signal(
            ticker="BARC.LSE", signal_type="INSIDER_BUY", confidence=0.9
        )

        assert signal["ticker"] == "BARC.LSE"
        assert signal["signal_type"] == "INSIDER_BUY"
        assert signal["confidence"] == 0.9
        assert "data" in signal
        assert "timestamp" in signal

    def test_sample_stock_factory(self, sample_stock):
        """Test stock factory creates valid stock data."""
        stock = sample_stock(ticker="LLOY.LSE", name="Lloyds Banking Group")

        assert stock["ticker"] == "LLOY.LSE"
        assert stock["name"] == "Lloyds Banking Group"
        assert stock["exchange"] == "LSE"
        assert stock["currency"] == "GBP"

    def test_mock_eodhd_client(self, mock_eodhd_client):
        """Test EODHD mock client fixture."""
        assert mock_eodhd_client is not None
        assert hasattr(mock_eodhd_client, "get_fundamentals")

    def test_mock_cityfalcon_client(self, mock_cityfalcon_client):
        """Test CityFALCON mock client fixture."""
        assert mock_cityfalcon_client is not None
        assert hasattr(mock_cityfalcon_client, "get_news")

    def test_mock_signal_bus(self, mock_signal_bus):
        """Test signal bus mock fixture."""
        assert mock_signal_bus is not None
        assert hasattr(mock_signal_bus, "publish")
        assert hasattr(mock_signal_bus, "subscribe")


# ============================================================================
# Sprint 0 Gate Check
# ============================================================================


@pytest.mark.slow
class TestSprintZeroGateCheck:
    """
    Sprint 0 completion gate check.

    All tests in this class MUST pass before Story 1.1 can begin.
    """

    def test_project_structure_exists(self):
        """Verify core project directories exist."""
        import os

        base_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        required_dirs = [
            "src",
            "src/agents",
            "src/data",
            "src/models",
            "src/schemas",
            "tests",
            "tests/unit",
            "tests/integration",
        ]

        for dir_path in required_dirs:
            full_path = os.path.join(base_path, dir_path)
            assert os.path.exists(full_path), f"Missing directory: {dir_path}"

    def test_requirements_file_exists(self):
        """Verify requirements.txt exists."""
        import os

        base_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        req_file = os.path.join(base_path, "requirements.txt")
        assert os.path.exists(req_file), "requirements.txt not found"

    def test_env_template_exists(self):
        """Verify .env.template exists."""
        import os

        base_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        env_template = os.path.join(base_path, ".env.template")
        assert os.path.exists(env_template), ".env.template not found"

    def test_gitignore_exists(self):
        """Verify .gitignore exists."""
        import os

        base_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        gitignore = os.path.join(base_path, ".gitignore")
        assert os.path.exists(gitignore), ".gitignore not found"

    def test_readme_exists(self):
        """Verify README.md exists with setup instructions."""
        import os

        base_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        readme = os.path.join(base_path, "README.md")
        assert os.path.exists(readme), "README.md not found"

        # Verify README contains setup instructions
        with open(readme, "r") as f:
            content = f.read()
            assert "setup" in content.lower(), "README missing setup instructions"

    def test_pytest_runs_successfully(self):
        """Verify pytest can run (this test validates itself!)."""
        assert True, "Pytest is working!"

    @pytest.mark.asyncio
    async def test_mock_llm_provider_available(self, mock_llm):
        """Verify Mock LLM provider is available for zero-cost testing."""
        response = await mock_llm.generate("Test")
        assert response.content is not None
        assert response.model == "mock-gpt-4"

    def test_sprint_zero_complete(self):
        """
        FINAL GATE CHECK: Sprint 0 is complete.

        If this test passes, Sprint 0 is COMPLETE and Story 1.1 can begin.
        """
        print("\n" + "=" * 70)
        print("âœ… SPRINT 0 COMPLETE - CLEARED FOR STORY 1.1")
        print("=" * 70)
        print("\nNext steps:")
        print("1. Run: /bmad:bmm:workflows:sprint-planning")
        print("2. Begin Story 1.1: Project Initialization & Structure")
        print("=" * 70)
        assert True
